{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 0\n",
    "words = dict()\n",
    "with open('words.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        words[line.strip()] = freq\n",
    "        freq += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = []\n",
    "current_passage = None\n",
    "skip_next = False\n",
    "\n",
    "with open('passages.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        \n",
    "\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "            \n",
    "\n",
    "        if line and line[0].isdigit() and '. ' in line:\n",
    "\n",
    "            passage_text = line[line.index('. ') + 2:]\n",
    "            passages.append(passage_text)\n",
    "            skip_next = True\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['melted', 'glaciers', 'wound', 'formed', 'mississippi'], ['caves', 'sandstone', 'chomped', 'softened', 'hard-packed'], ['caves', 'dripping', 'sandstone', 'cannot', 'breathe'], ['caves', 'sandstone', 'cliffs', 'caves’', 'carving'], ['caves', 'lyman', 'sandstone', 'usefulness', 'dayton'], ['frenchmen', 'caves', 'sandstone', 'carved', 'crumbled'], ['frenchmen', 'caves', 'tasty', 'manure', 'mushrooms'], ['frenchmen', 'caves', '“castle', 'royal”', 'mushrooms'], ['sang', 'cave', 'musicians', 'makers', 'cheese'], ['minnesota’s', 'caves', 'settlers', 'dreamed', 'paul’s'], ['tallest', 'taller', 'skyscrapers', 'elevators', 'skyscraper'], ['wasn’t', 'tallest', 'skyscrapers', 'petronas', 'towers'], ['bricks', 'skyscraper', 'beams', 'collapse', 'tall'], ['skeleton', 'holds', 'frame', 'building', 'just'], ['bolted', 'hammered', 'skyscraper', 'pilings', 'floors'], ['cranes', 'wrestle', 'collapsing', 'earthquakes', 'ironworkers'], ['fir', 'bolts', 'ironworkers', 'tighten', 'curtains'], ['decorate', 'wires', 'pipes', 'paint', 'install'], ['whisk', 'triple-decker', 'skyscrapers', 'elevators', 'don’t'], ['climbed', 'lakesha', 'silence', 'breath', 'singing'], ['practicing', 'lakesha', 'loud', 'admit', 'singing'], ['smiled', 'earplugs', 'chuckled', 'farther', 'amazingly'], ['lively', 'laughed', 'wore', 'lakesha', 'jumping'], ['paints', 'quietly', 'see—he', 'pastels', 'pencils'], ['trunks', 'maria’s', 'thicker', 'farther', 'paintbrush'], ['smiling', 'ramos', 'embarrassment', 'strokes', 'clap'], ['imagined', 'yearned', 'ugly', 'karen', 'waters'], ['sliced', 'swirled', 'loomed', 'chilly', 'shaggy'], ['bedtime', 'upward', 'seagull', 'sounded', 'swoosh'], ['earthy', 'crinkly', 'unattractive', 'seaweed', 'crisp'], ['patel', 'classmates', 'unsure', 'seaweed', 'bent'], ['clump', 'pooled', 'shallow', 'seaweed', 'pale'], ['clump', 'seahorse', 'patel', 'shouted', 'seaweed'], ['smiled', 'clinging', 'half-smile', 'enchanted', 'ragged'], ['seahorse', 'patel', 'scooted', 'delighted', 'submarine'], ['mussels', 'rov’s', 'shellfish', 'muddy', 'headlights'], ['lake’s', 'ripples', 'seafloor', 'rov', 'spreading'], ['seafloor', 'shoreline', 'salty', 'lakes', 'wave'], ['leaks', 'super-salty', 'dissolves', 'layers', 'lie'], ['heavier', 'gathers', 'super-salty', 'saltier', 'dense'], ['barren', 'seafloor', 'pitch-black', 'sunlight', 'lakes'], ['divers', 'underwater', 'submarines', 'traveled', 'brine'], ['mussels', 'lifeless', 'worms', 'seafloor', 'vents'], ['confidently', 'campsite', 'stephan', 'scary', 'wanting'], ['flattened', 'pots', 'marshmallows', 'grabbed', 'skateboards']]\n"
     ]
    }
   ],
   "source": [
    "chosen_words = []\n",
    "\n",
    "for passage in passages:\n",
    "    word_value_paired = set()\n",
    "    for word in passage.lower().split():\n",
    "        cleaned_word = word.strip('.,!?:;()[]{}\"\\'-1234567890')\n",
    "\n",
    "        if not cleaned_word:\n",
    "            continue\n",
    "        if cleaned_word in words:\n",
    "            word_value_paired.add((cleaned_word, words[cleaned_word]))\n",
    "        else:\n",
    "            word_value_paired.add((cleaned_word, 10000))\n",
    "    word_value_paired = list(word_value_paired)\n",
    "    word_value_paired.sort(key=lambda x: x[1], reverse=True)\n",
    "    chosen_words.append([word_value_paired[x][0] for x in range(5)])\n",
    "print(chosen_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_frequencies(passages):\n",
    "    \"\"\"\n",
    "    Count the frequency of each word across all passages.\n",
    "    \n",
    "    Args:\n",
    "        passages (list): List of passage strings\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with words as keys and their frequencies as values\n",
    "    \"\"\"\n",
    "    # Join all passages into a single string\n",
    "    all_text = ' '.join(passages)\n",
    "    \n",
    "    # Convert to lowercase and split into words\n",
    "    # Use regex to remove punctuation and keep only alphanumeric words\n",
    "    words = re.findall(r'\\b[a-zA-Z0-9]+\\b', all_text.lower())\n",
    "    \n",
    "    # Count frequencies using Counter\n",
    "    word_frequencies = Counter(words)\n",
    "    \n",
    "    # Convert Counter to regular dictionary if needed\n",
    "    return dict(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_word_freq = count_word_frequencies(passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage_word_freq['minnesota']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(passage_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_words = []\n",
    "passage_num = 0\n",
    "incorrect_words_list = []\n",
    "for chosen in chosen_words:\n",
    "    word_value_paired = set()\n",
    "    words_in_this_passage = set()\n",
    "    for word in chosen:\n",
    "        cleaned_word = word.strip('.,!?:;()[]{}\"\\'-1234567890')\n",
    "\n",
    "        if not cleaned_word:\n",
    "            continue\n",
    "        passage_here = [passages[i] for i in range(len(passages)) if i != passage_num]\n",
    "        passage_word_freq = count_word_frequencies(passage_here)\n",
    "        list_word_freq = set(passage_word_freq.keys())\n",
    "        for w in passages[passage_num].lower().split():\n",
    "            cw = w.strip('.,!?:;()[]{}\"\\'-1234567890')\n",
    "            words_in_this_passage.add(cw)\n",
    "        incorrect_words_set = list_word_freq - words_in_this_passage\n",
    "        incorrect_words_set = list(incorrect_words_set)\n",
    "        r = random.sample((range(len(incorrect_words_set))), 3)\n",
    "        incorrect_words = [incorrect_words_set[i] for i in r]\n",
    "        incorrect_words_list.append(incorrect_words)\n",
    "        if cleaned_word in passage_word_freq:\n",
    "            word_value_paired.add((cleaned_word, passage_word_freq[cleaned_word]))\n",
    "    word_value_paired = list(word_value_paired)\n",
    "    word_value_paired.sort(key=lambda x: x[1])\n",
    "    final_words.append([word_value_paired[x][0] for x in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['formed', 'mississippi'],\n",
       " ['chomped', 'softened'],\n",
       " ['breathe', 'dripping'],\n",
       " ['cliffs', 'carving'],\n",
       " ['usefulness', 'lyman'],\n",
       " ['crumbled', 'carved'],\n",
       " ['tasty', 'mushrooms'],\n",
       " ['mushrooms', 'frenchmen'],\n",
       " ['makers', 'musicians'],\n",
       " ['settlers', 'dreamed'],\n",
       " ['taller', 'tallest'],\n",
       " ['petronas', 'towers'],\n",
       " ['bricks', 'collapse'],\n",
       " ['skeleton', 'holds'],\n",
       " ['hammered', 'bolted'],\n",
       " ['wrestle', 'collapsing'],\n",
       " ['fir', 'bolts'],\n",
       " ['pipes', 'install'],\n",
       " ['whisk', 'elevators'],\n",
       " ['climbed', 'silence'],\n",
       " ['loud', 'admit'],\n",
       " ['chuckled', 'amazingly'],\n",
       " ['laughed', 'wore'],\n",
       " ['quietly', 'paints'],\n",
       " ['paintbrush', 'thicker'],\n",
       " ['ramos', 'strokes'],\n",
       " ['ugly', 'yearned'],\n",
       " ['swirled', 'sliced'],\n",
       " ['upward', 'seagull'],\n",
       " ['unattractive', 'crinkly'],\n",
       " ['unsure', 'bent'],\n",
       " ['pale', 'pooled'],\n",
       " ['shouted', 'clump'],\n",
       " ['ragged', 'enchanted'],\n",
       " ['delighted', 'submarine'],\n",
       " ['muddy', 'shellfish'],\n",
       " ['ripples', 'spreading'],\n",
       " ['wave', 'shoreline'],\n",
       " ['dissolves', 'leaks'],\n",
       " ['gathers', 'saltier'],\n",
       " ['barren', 'sunlight'],\n",
       " ['divers', 'submarines'],\n",
       " ['worms', 'vents'],\n",
       " ['scary', 'campsite'],\n",
       " ['grabbed', 'pots']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = dict()\n",
    "incorrect = dict()\n",
    "for i in range(len(final_words)):\n",
    "    final_dict[f'{i+1}'] = final_words[i]\n",
    "    incorrect[f'{i+1}'] = incorrect_words_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('words.json','w') as json_file:\n",
    "    json.dump(final_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('incorrect.json','w') as json_file:\n",
    "    json.dump(incorrect, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tele_statistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84a6233198c34f03b3c386cf553600c9fdf7deb3e657306515660cd49e365240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
